{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "### `! git clone https://github.com/ds3001f25/wrangling_assignment.git`\n",
        "### Do Q1 and Q2\n",
        "### Reading material: `tidy_data.pdf`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q1.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the Minnesota police use of for data, `./data/mn_police_use_of_force.csv`, clean the `subject_injury` variable, handling the NA's; this gives a value `Yes` when a person was injured by police, and `No` when no injury occurred. What proportion of the values are missing? Is this a concern? Cross-tabulate your cleaned `subject_injury` variable with the `force_type` variable. Are there any patterns regarding when the data are missing?\n",
        "3. Dummy variable: For the pretrial data covered in the lecture `./data/justice_data.parquet`, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "9d412a8d",
      "metadata": {
        "collapsed": true,
        "id": "9d412a8d",
        "outputId": "da296031-88b1-4a45-e823-2caaae7a8eb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  145,    37,    28,   199,   549,   149,   250,    90,   270,\n",
              "         290,   170,    59,    49,    68,   285,    75,   100,   150,\n",
              "         700,   125,   175,    40,    89,    95,    99,   499,   120,\n",
              "          79,   110,   180,   143,   230,   350,   135,    85,    60,\n",
              "          70,    55,    44,   200,   165,   115,    74,    84,   129,\n",
              "          50,   185,    80,   190,   140,    45,    65,   225,   600,\n",
              "         109,  1990,    73,   240,    72,   105,   155,   160,    42,\n",
              "         132,   117,   295,   280,   159,   107,    69,   239,   220,\n",
              "         399,   130,   375,   585,   275,   139,   260,    35,   133,\n",
              "         300,   289,   179,    98,   195,    29,    27,    39,   249,\n",
              "         192,   142,   169,  1000,   131,   138,   113,   122,   329,\n",
              "         101,   475,   238,   272,   308,   126,   235,   315,   248,\n",
              "         128,    56,   207,   450,   215,   210,   385,   445,   136,\n",
              "         247,   118,    77,    76,    92,   198,   205,   299,   222,\n",
              "         245,   104,   153,   349,   114,   320,   292,   226,   420,\n",
              "         500,   325,   307,    78,   265,   108,   123,   189,    32,\n",
              "          58,    86,   219,   800,   335,    63,   229,   425,    67,\n",
              "          87,  1200,   158,   650,   234,   310,   695,   400,   166,\n",
              "         119,    62,   168,   340,   479,    43,   395,   144,    52,\n",
              "          47,   529,   187,   209,   233,    82,   269,   163,   172,\n",
              "         305,   156,   550,   435,   137,   124,    48,   279,   330,\n",
              "        5000,   134,   378,    97,   277,    64,   193,   147,   186,\n",
              "         264,    30,  3000,   112,    94,   379,    57,   415,   236,\n",
              "         410,   214,    88,    66,    71,   171,   157,   545,  1500,\n",
              "          83,    96,  1800,    81,   188,   380,   255,   505,    54,\n",
              "          33,   174,    93,   740,   640,  1300,   440,   599,   357,\n",
              "        1239,   495,   127,  5999,   178,   348,   152,   242,   183,\n",
              "         253,   750,   259,   365,   273,   197,   397,   103,   389,\n",
              "         355,   559,    38,   203,   999,   141,   162,   333,   698,\n",
              "          46,   360,   895,    10,    41,   206,   281,   449,   388,\n",
              "         212,   102,   201,  2750,  4750,   432,   675,   167,   390,\n",
              "         298,   339,   194,   302,   211,   595,   191,    53,   361,\n",
              "         480,  8000,  4500,   459,   997,   345,   216,   218,   111,\n",
              "         735,   276,    91,   490,   850,   398,    36,   775,   267,\n",
              "         625,   336,  2500,   176,   725,  3750,   469,   106,   460,\n",
              "         287,   575,   227,   263,    25,   228,   208,   177,   880,\n",
              "         148,   116,   685,   470,   217,   164,    61,   645,   699,\n",
              "         405,   252,   319,   268,   419,   343,   525,   311,   840,\n",
              "         154,   294,   950,   409,   184,   257,   204,   241,  2000,\n",
              "         412,   121,   288,   196,   900,   647,   524,  1750,   309,\n",
              "         510,  1495,  1700,   799,   383,   372,   492,   327,  1999,\n",
              "         656,   224,   173,   875,  1170,   795,   690,   146,   465,\n",
              "        1100,   151,   274,   429,   825,   282,   256,  1111,   620,\n",
              "         271,   161,    51,   855,   579,  1174,   430,    20,   899,\n",
              "         649,   485,   181,   455,  4000,   243,   342,   590,   560,\n",
              "         374,   437,   232,   359,   985,    31,   244,   254,   723,\n",
              "         237,   428,   370,    34,  1400,   580,  2520,   221,   749,\n",
              "        1600,  2695,   306,   202,   680,   570,   520,   223,  2295,\n",
              "         213,  1065,   346,    24,   286,   296,   266,    26,   995,\n",
              "        1368,   393,   182,   635,   258,   780,   589,   347,  1250,\n",
              "        1350,   446,  3200,  1050,  1650,  1550,   975,   323,  6500,\n",
              "        2499,  1850,  2250,   715,   461,   540,   356,   439,   384,\n",
              "         569,  1900,    22,   785,   626,   830,   318,   444,   321,\n",
              "         401,  1499,   888,   369,   770,   386,   366,   344,   630,\n",
              "         313,   597,   262,   509, 10000,   278,   312,   789,  1195,\n",
              "         422,    21,   765,  3500,   945,   326,  3100,  2486,  3390,\n",
              "        1356,  2599,   472,   454,   328,   396,   291])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/Airbnb_Homework.csv')\n",
        "df_clean = df.copy() #Never edit the original, always copy\n",
        "df_clean[\"Price\"] = df_clean[\"Price\"].str.replace(\",\", \"\") #This is replacing the comma from the string with a zero space because when there is a comma present in the value python treats it as a string and not an integer\n",
        "df_clean[\"Price\"] = pd.to_numeric(df_clean[\"Price\"]) #This then is pushing the strings to integers, so then you can apply math functions to this column\n",
        "df_clean[\"Price\"].isna().sum() #Making sure there are no NAs in this column, from the output there aren't any\n",
        "df_clean[\"Price\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df2 = pd.read_csv('/MN_Police_Use_of_Force.csv')\n",
        "df_clean2 = df2.copy()\n",
        "missing_count = df_clean2['subject_injury'].isna().sum()\n",
        "total_count = len(df_clean2[\"subject_injury\"])\n",
        "missing_proportion = missing_count / total_count\n",
        "print(missing_proportion) # From the output of this, it shows that 76% of the data is missing, therefore this is concerning given the size of the data\n",
        "crosstab = pd.crosstab(df_clean2['subject_injury'], df_clean2['force_type'], dropna= False)\n",
        "print(crosstab) #The data seems to be missing on a much larger scale when there is a bodily force, chemical irritant, or taser involved. In some cases, even there is no data present for the type of force, as less lethal or maximal restraint technique"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Cb3b6l9ulzn9",
        "outputId": "3672f92f-db2c-4af5-989c-e1c6f7c2c632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Cb3b6l9ulzn9",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7619342359767892\n",
            "force_type      Baton  Bodily Force  Chemical Irritant  Firearm  \\\n",
            "subject_injury                                                    \n",
            "No                  0          1093                131        2   \n",
            "Yes                 2          1286                 41        0   \n",
            "NaN                 2          7051               1421        0   \n",
            "\n",
            "force_type      Gun Point Display  Improvised Weapon  Less Lethal  \\\n",
            "subject_injury                                                      \n",
            "No                             33                 34            0   \n",
            "Yes                            44                 40            0   \n",
            "NaN                            27                 74           87   \n",
            "\n",
            "force_type      Less Lethal Projectile  Maximal Restraint Technique  \\\n",
            "subject_injury                                                        \n",
            "No                                   1                            0   \n",
            "Yes                                  2                            0   \n",
            "NaN                                  0                          170   \n",
            "\n",
            "force_type      Police K9 Bite  Taser  \n",
            "subject_injury                         \n",
            "No                           2    150  \n",
            "Yes                         44    172  \n",
            "NaN                         31    985  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df3 = pd.read_parquet('/Justice_Data.parquet')\n",
        "df_clean3 = df3.copy()\n"
      ],
      "metadata": {
        "id": "4q7zE5yBmTfj"
      },
      "id": "4q7zE5yBmTfj",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5a60a44e",
      "metadata": {
        "id": "5a60a44e"
      },
      "source": [
        "**Q2.** Go to https://sharkattackfile.net/ and download their dataset on shark attacks (Hint: `GSAF5.xls`).\n",
        "\n",
        "1. Open the shark attack file using Pandas. It is probably not a csv file, so `read_csv` won't work.\n",
        "2. Drop any columns that do not contain data.\n",
        "3. Clean the year variable. Describe the range of values you see. Filter the rows to focus on attacks since 1940. Are attacks increasing, decreasing, or remaining constant over time?\n",
        "4. Clean the Age variable and make a histogram of the ages of the victims.\n",
        "5. What proportion of victims are male?\n",
        "6. Clean the `Type` variable so it only takes three values: Provoked and Unprovoked and Unknown. What proportion of attacks are unprovoked?\n",
        "7. Clean the `Fatal Y/N` variable so it only takes three values: Y, N, and Unknown.\n",
        "8. Are sharks more likely to launch unprovoked attacks on men or women? Is the attack more or less likely to be fatal when the attack is provoked or unprovoked? Is it more or less likely to be fatal when the victim is male or female? How do you feel about sharks?\n",
        "9. What proportion of attacks appear to be by white sharks? (Hint: `str.split()` makes a vector of text values into a list of lists, split by spaces.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}